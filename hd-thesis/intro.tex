The cryptographic algorithms that protect our data in the Internet age are not, by and large, developed by cryptographers. Instead, many Internet applications rely on cryptographic standards for guidance. These standards documents are published by organizations like the National Institute of Standards and Technology (NIST) and the Internet Engineering Task Force (IETF) as authoritative references on how to implement secure cryptography. Standards ensure interoperability between Internet applications, and give developers a trusted source for their cryptographic needs. Standardized algorithms like the Transport Layer Security protocol (TLS 1.3) currently protect roughly to 85 percent of all Internet traffic~\cite{fortinet}.

Because standardized cryptography is often used at large scale, any vulnerabilities have significant consequences. Furthermore, adopting a new standard is a slow, expensive process, so updates and patches are relatively rare. Before standards are published, they therefore undergo a vigorous vetting process, complete with extensive public scrutiny. In recent years, this process often includes formal proofs of security among other validation methods. In this work, we provide new and improved proofs of security for several current and future cryptographic standards.

Security proofs establish bounds on the success probability of an adversary interacting with a target scheme in an abstract model that defines the attack surface. The exact limit on this probability depends both on the resources of the adversary and on the security of any underlying cryptographic primitives or mathematical assumptions. If a scheme's security is close to that of its components' security for all resource levels, we say that the bounds are ``tight''. Tight bounds can be used to help select parameter sizes for cryptographic components; other bounds may provide heuristic guarantees about a scheme's security. Once a scheme has a valid security proof, an attacker can only successfully attack it with high probability by violating the assumptions made by the proof or model, or by using enough resources to vacate the bounds.

We consider the existing proofs for current and future standards, and identify certain ways they do not rule out attacks: loose bounds and gaps between abstract threat models and implementations. Wherever possible, we seek to repair the existing proofs or leverage prior work in a modular way, rather than replace them entirely. 

\headingb{Hash functions, indifferentiability and the ROM} 

One place where many proofs break down is in their treatment of hash functions. The random oracle model (ROM) of Bellare and Rogaway~\cite{BR93} is a powerful model in which hash functions are treated as publicly accessible random functions, often with infinite domains. Of course, such functions are unrealizable, and thus proofs in the ROM offer only heuristic evidence of security. However, the ROM is a commonly used assumption, relied on by security proofs for many standards~\cite{JC:DieJag21, EC:PoiSte96, SP:BCJZ21, EC:BDPV08, C:CDMP05} and other widely-used cryptographic primitives, and there are few natural examples of schemes which are secure in the ROM but insecure in practice.

Not all hash functions can be suitably modeled as random oracles. The standardized hash functions are constructed by iterating an underlying compression function or random permutation, and it is essential to make sure that this underlying structure does not admit additional vulnerabilities. Maurer et al. developed the indifferentiability framework, which can be used to evaluate whether a particular construction can be used to securely instantiate a random oracle~\cite{TCC:MauRenHol04}. They proved a powerful composition theorem. If a scheme is proven secure (for most common definitions of security) in the random oracle model, and it is instantiated with an indifferentiable construction from some compression function, then the scheme is also secure when only the compression function is modeled as a random oracle. 

\headingb{Key encapsulation mechanisms}

We begin in Chapter~\ref{chap:domsep} with a case study of the ongoing NIST standardization process for post-quantum key encapsulation mechanisms (KEMs). Of the initial, now-eliminated, candidates, we identify highly efficient key recovery attacks on three schemes. These attacks fall in a gap between a security model with three independent random oracles, and implementations which instantiate them using a single (indifferentiable) hash function. Because our attacks circumvent the candidates' security proofs rather contradicting them, they went unnoticed for more than a year of intense public scrutiny as proposed standards. 

The failure of the attacked schemes was in a task we call oracle cloning: constructing multiple independent random oracles given access to a single RO. We highlight thirteen other candidate KEM schemes which do not approach oracle cloning with care and whose proofs also exhibit gaps, and ten schemes that performed oracle cloning well. We then collect a library of simple and secure oracle cloning techniques, including domain separation, and validate them in a new framework called read-only indifferentiability. Using these results, we extend the existing proofs of twelve of the thirteen questionable schemes to cover their oracle cloning methods, thus closing the gap. The thirteenth scheme was updated in a subsequent round of the standardization process to use one of our techniques~\cite{nistpqc:NewHope}. 

\headingb{Authenticated key exchange}

Over the next two chapters, we study the Transport Layer Security 1.3 Handshake Protocol~\cite{rfc8446}. This protocol is used establish secret, pseudorandom session keys for billions of Internet connections per day. As part of its standardization process, the handshake protocol received its first proof of security from Dowling et al.~\cite{CCS:DFGS15} in 2015. Although this proof provides heuristic evidence of security for the handshake protocol, we empirically demonstrate in Chapters~\ref{chap:tight-ake} and~\ref{chap:tight-ake-ext} (for the full handshake and pre-shared key modes respectively) that its bounds are too loose to justify the standardized parameter sets for global usage scales.

The quadratic loss in the number of sessions in the Dowling bound is common to many contemporary proofs for authenticated key exchange protocols based on the Diffie--Hellman (DH) problem. The first fully tight bounds for this style of key exchange were given by Cohn-Gordon et al.~\cite{C:CCGJJ19} for a custom-designed key exchange protocol. The cost of this advancement was a change in assumption: the Cohn-Gordon proof relied on the interactive Strong DH assumption rather than more standard noninteractive DH assumptions. 

In Chapter~\ref{chap:tight-ake}, we apply the Cohn-Gordon technique to the full TLS~1.3 handshake protocol and to the SIGMA key exchange protocol~\cite{C:Krawczyk03} and achieve a full justification of standardized parameter sets. We also justify the change of assumption in two ways: by evaluating the hardness of Strong DH in the generic group model, and by highlighting that the proof of Dowling et al. also assumes Strong DH implicitly. Diemert and Jager~\cite{JC:DieJag20} gave a concurrent and independent analysis of the TLS~1.3 handshake with similar final bounds.

In Chapter~\ref{chap:tight-ake-ext}, we build on the work of Chapter~\ref{chap:tight-ake} and that of Diemert and Jager to tightly prove security for the pre-shared key modes of the TLS~1.3 handshake protocol. As an intermediate step, we establish the first justification of the TLS 1.3 key schedule in the indifferentiability framework. This approach is not only more rigorous than previous abstractions; it also simplifies the remaining proof and helps establish independence for the derived keys. However, we also highlight an obstacle in the poor domain separation of the key schedule that prevents an indifferentiability proof for one choice of mode and hash function (PSK-only mode with $\SHA{384}$). Finally, we treat handshake encryption as a modular transform applied to a generic key exchange protocol and provide general results on the composition of such a transform.

\headingb{EdDSA signatures}
We address the EdDSA signature scheme~\cite{JCEng:BDLSY12} in Chapter~\ref{chap:eddsa}. EdDSA is a tweaked variant of the Schnorr signature scheme~\cite{C:Schnorr89} that hardens it against randomness reuse and certain side-channel attacks. It's a standardized signature algorithm for TLS~1.3, and is also used by many blockchain applications and encrypted messaging services, including WhatsApp and Signal.

Over the years, Schnorr signatures have received several proofs of security~\cite{JC:PoiSte00}, including some recent tighter proofs from non-standard assumptions~\cite{INDOCRYPT:BelDai20, EC:FucPloSeu20}. Ed25519, however, was first proven secure in 2020 by Brendel et al.~\cite{SP:BCJZ21}. Like the initial proofs of Schnorr signatures, their reduction is not tight and models its hash function as a random oracle. The latter quality presents a concern because Ed25519 uses SHA512, an MD-style hash function which is known to be differentiable from a random oracle~\cite{C:CDMP05} and subject to length-extension attacks.

We define a generic transform called Derive-then-Derandomize, that captures the hardening tweaks applied by Bernstein et al. for EdDSA. We prove that it works from standard assumptions. We then give a general lemma showing indifferentiability of $\ShrinkMD$, a class of constructions that apply a shrinking output transform to an Merkle-Damgard-style hash function. The particular usage of $\SHA{512}$ within Ed25519 falls within this class. Using these, we give a direct, fully tight reduction from EdDSA signatures to Schnorr signatures. Our proof enables tighter bounds for EdDSA that leverage both historic trust and recent analysis of Schnorr; it also captures the use of $\SHA{512}$ as a hash function and includes length-extension attacks in its threat model.

\headingb{Verifiable Distributed Aggregation Functions}

Finally in chapter 5, we make the first provable security contribution to an ongoing standardization process.
The IETF's working group on privacy preserving measurement (PPM)~\cite{ppm}, in their draft standard, defines a class of cryptographic primitives called ``Verifiable Distributed Aggregation Functions (VDAFs)''~\cite{draft-irtf-cfrg-vdaf-03}. VDAFs are a class of multi-party computation protocols that enable a collector, with the help of several third-party aggregators, to learn an aggregate statistic about a population of clients without compromising the privacy of individual client measurements. The Prio protocol by Corrigan-Gibbs and Boneh~\cite{CGB17}, an example of the VDAF paradigm has already been used at global scale as part of the Exposure Notification Private Analytics (ENPA) program during the Covid-19 pandemic~\cite{ENPrivateAnalytics}. 

We give the first provable security treatment for VDAFs, This includes a formal framework of syntax and game-based definitions capturing privacy, robustness, and correctness, and analysis of two constructions within this framework.
The first is Prio3, a variant of Prio incorporating optimizations by Boneh et al.~\cite{BBCG+19} and a candidate for standardization within the PPM draft. The second, called Doplar, we introduce as a way to reduce the round complexity of the Poplar system of Boneh et al.~\cite{BBCG+21},
itself a candidate for standardization. To achieve this improvement, Doplar requires slightly greater overall
bandwidth and computation.
